#!/usr/bin/env python3

import argparse
import io
import logging
import sys

import tacl


def main ():
    parser = generate_parser()
    args = parser.parse_args()
    configure_logging(args.verbose)
    try:
        args.func(args)
    except Exception as err:
        print('Error: {0}'.format(err))
        sys.exit(1)

def add_common_arguments (parser):
    """Adds common arguments for all parsers."""
    help = 'display debug information; multiple -v options increase the verbosity'
    parser.add_argument('-v', '--verbose', action='count', help=help)

def add_db_arguments (parser):
    """Adds common arguments for the database subcommands to `parser`."""
    parser.add_argument('-m', '--memory', action='store_true',
                        help='use RAM for temporary database storage')
    parser.add_argument('-r', '--ram', default=3, type=int,
                        help='number of gigabytes of RAM to use')
    parser.add_argument('db', metavar='DATABASE', help='path to database file')
    parser.add_argument('corpus', help='path to corpus', metavar='CORPUS')

def add_query_arguments (parser):
    parser.add_argument('catalogue', help='path to catalogue',
                        metavar='CATALOGUE')

def configure_logging (verbose):
    logger = logging.getLogger('tacl')
    if not verbose:
        log_level = logging.WARNING
    elif verbose == 1:
        log_level = logging.INFO
    else:
        log_level = logging.DEBUG
    logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s',
                        level=log_level)

def generate_parser ():
    parser = argparse.ArgumentParser(
        description='Analyse the text of corpora in various simple ways.')
    subparsers = parser.add_subparsers(title='subcommands')
    generate_catalogue_subparser(subparsers)
    generate_counts_subparser(subparsers)
    generate_diff_subparser(subparsers)
    generate_intersect_subparser(subparsers)
    generate_ngrams_subparser(subparsers)
    generate_report_subparser(subparsers)
    generate_strip_subparser(subparsers)
    return parser

def generate_catalogue (args):
    catalogue = tacl.Catalogue()
    catalogue.generate(args.corpus, args.label)
    catalogue.save(args.catalogue)

def generate_catalogue_subparser (subparsers):
    parser = subparsers.add_parser(
        'catalogue', description='Generate a catalogue file',
        help='generate a catalogue file')
    add_common_arguments(parser)
    parser.set_defaults(func=generate_catalogue)
    parser.add_argument('corpus', help='path to corpus')
    parser.add_argument('catalogue', help='path to catalogue file')
    parser.add_argument('-l', '--label', help='label to use for all texts',
                        default='')

def generate_counts_subparser (subparsers):
    parser = subparsers.add_parser(
        'counts', description='List counts of n-grams in each labelled text',
        help='list counts of n-grams in each labelled text, by size')
    parser.set_defaults(func=ngram_counts)
    add_common_arguments(parser)
    add_db_arguments(parser)
    add_query_arguments(parser)

def generate_diff_subparser (subparsers):
    parser = subparsers.add_parser(
        'diff', description='List n-grams unique to each sub-corpus.',
        help='list n-grams in one corpus but not the other')
    parser.set_defaults(func=ngram_diff)
    add_common_arguments(parser)
    add_db_arguments(parser)
    add_query_arguments(parser)

def generate_intersect_subparser (subparsers):
    parser = subparsers.add_parser(
        'intersect', description='List n-grams common to all sub-corpora.',
        help='list n-grams common to all sub-corpora')
    parser.set_defaults(func=ngram_intersection)
    add_common_arguments(parser)
    add_db_arguments(parser)
    add_query_arguments(parser)

def generate_ngrams (args):
    corpus = get_corpus(args)
    corpus.generate_ngrams(args.min_size, args.max_size, args.index)

def generate_ngrams_subparser (subparsers):
    parser = subparsers.add_parser(
        'ngrams', description='Generate n-grams from a corpus.',
        help='generate n-grams from a corpus')
    parser.set_defaults(func=generate_ngrams)
    add_common_arguments(parser)
    add_db_arguments(parser)
    parser.add_argument('-i', '--index', action='store_true',
                        help='drop indices before adding n-grams')
    parser.add_argument('min_size', metavar='MINIMUM_SIZE', type=int,
                        help='minimum size of n-gram to generate (integer)')
    parser.add_argument('max_size', metavar='MAXIMUM_SIZE', type=int,
                        help='maximum size of n-gram to generate (integer)')

def generate_report (args):
    if args.results == '-':
        results = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8', newline='')
    else:
        results = open(args.results, 'r', encoding='utf-8', newline='')
    report = tacl.Report(results)
    if args.remove:
        report.remove_label(args.remove)
    if args.min_texts or args.max_texts:
        report.prune_by_text_count(args.min_texts, args.max_texts)
    if args.reduce:
        report.reduce()
    if args.min_size or args.max_size:
        report.prune_by_ngram_size(args.min_size, args.max_size)
    if args.min_count or args.max_count:
        report.prune_by_ngram_count(args.min_count, args.max_count)
    output(report.csv())

def generate_report_subparser (subparsers):
    parser = subparsers.add_parser(
        'report', description='Generate a report on a query results file',
        help='generate a report on a query results file')
    add_common_arguments(parser)
    parser.set_defaults(func=generate_report)
    parser.add_argument('--min-count', dest='min_count',
                        metavar='MINIMUM_COUNT', type=int,
                        help='minimum total count of n-gram to include')
    parser.add_argument('--max-count', dest='max_count',
                        metavar='MAXIMUM_COUNT', type=int,
                        help='maximum total count of n-gram to include')
    parser.add_argument('--min-size', dest='min_size', metavar='MINIMUM_SIZE',
                        type=int, help='minimum size of n-grams to include')
    parser.add_argument('--max-size', dest='max_size', metavar='MAXIMUM_SIZE',
                        type=int, help='maximum size of n-grams to include')
    parser.add_argument('--min-texts', dest='min_texts',
                        metavar='MINIMUM_TEXT_COUNT', type=int,
                        help='minimum count of texts containing n-gram to include')
    parser.add_argument('--max-texts', dest='max_texts',
                        metavar='MAXIMUM_TEXT_COUNT', type=int,
                        help='maximum count of texts containing n-gram to include')
    parser.add_argument('--reduce', action='store_true',
                        help='remove n-grams that are contained in larger n-grams')
    parser.add_argument('--remove', metavar='LABEL', type=str,
                        help='remove labelled results')
    parser.add_argument('results', metavar='RESULTS',
                        help='path to CSV results; use - for stdin')

def generate_strip_subparser (subparsers):
    help='preprocess a corpus by stripping unwanted material from each text'
    description='Preprocess a corpus by stripping unwanted material from each text.'
    parser = subparsers.add_parser('strip', description=description, help=help)
    parser.set_defaults(func=strip_texts)
    add_common_arguments(parser)
    parser.add_argument('input', metavar='INPUT',
                        help='directory containing files to strip')
    parser.add_argument('output', metavar='OUTPUT',
                        help='directory to output stripped files to')

def get_corpus (args):
    manager = tacl.DBManager(args.db, args.memory, args.ram)
    corpus = tacl.Corpus(args.corpus, manager)
    return corpus

def get_catalogue (args):
    catalogue = tacl.Catalogue()
    catalogue.load(args.catalogue)
    return catalogue

def ngram_counts (args):
    corpus = get_corpus(args)
    catalogue = get_catalogue(args)
    output(corpus.counts(catalogue))

def ngram_diff (args):
    corpus = get_corpus(args)
    catalogue = get_catalogue(args)
    output(corpus.diff(catalogue))

def ngram_intersection (args):
    corpus = get_corpus(args)
    catalogue = get_catalogue(args)
    output(corpus.intersection(catalogue))

def output (fh):
    """Output the content of `fh` to stdout.

    :param fh: filehandle containing data
    :type data: `io.StringIO`

    """
    sys.stdout.buffer.write(fh.getvalue().encode('utf-8'))
    logging.info('Finished outputting results')

def strip_texts (args):
    stripper = tacl.Stripper(args.input, args.output)
    stripper.strip_files()


if __name__ == '__main__':
    main()
